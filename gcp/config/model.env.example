# vLLM runtime config (read by container/entrypoint.sh)
# Copy this file to model.env and customize for your deployment

MODEL_ID=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
OPTION_MAX_MODEL_LEN=8192
OPTION_GPU_MEMORY_UTILIZATION=0.95
OPTION_ENFORCE_EAGER=true
MAX_BATCH_TOTAL_TOKENS=32768
VLLM_PORT=8000
PORT=8080

# Optional: Override model repository if needed
# MODEL_REPO=your-custom-repo
